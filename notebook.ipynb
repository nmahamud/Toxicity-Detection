{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "vhf_cTWk3dgj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import RobertaTokenizerFast\n",
        "from collections import Counter\n",
        "import re\n",
        "import csv\n",
        "\n",
        "import sklearn\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
        "import pandas as pd\n",
        "from sklearn.metrics import multilabel_confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "RJrqRvpa35Lv"
      },
      "outputs": [],
      "source": [
        "NUM_CLASSES = 6\n",
        "NUM_SENTIMENT = 3\n",
        "NUM_HATE = 2\n",
        "\n",
        "BATCH_SIZE=8\n",
        "EPOCHS=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "flN6K8yg3nKf"
      },
      "outputs": [],
      "source": [
        "trainDatasetPath = './train.csv'\n",
        "\n",
        "class ToxicCommentsRobertaDataset(Dataset):\n",
        "    def __init__(self, path, columnName, tokenizer, maxLength=512):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            path (string): Path to the csv file with annotations.\n",
        "            columnName (string): Name of the column containing the comments.\n",
        "            tokenizer: Tokenizer to use for tokenizing the comments.\n",
        "            maxLength (int): Maximum length of the tokenized comments.\n",
        "        \"\"\"\n",
        "        self.texts = []\n",
        "        self.labels_list = []\n",
        "        self.tokenizer = tokenizer\n",
        "        self.maxLength = maxLength\n",
        "        with open(path, encoding='utf-8') as f:\n",
        "            reader = csv.reader(f)\n",
        "            header = next(reader)\n",
        "            textColIndex = header.index(columnName)\n",
        "            labelColIndices = [\n",
        "                header.index(label)\n",
        "                for label in [\n",
        "                    \"toxic\",\n",
        "                    \"severe_toxic\",\n",
        "                    \"obscene\",\n",
        "                    \"threat\",\n",
        "                    \"insult\",\n",
        "                    \"identity_hate\",\n",
        "                ]\n",
        "            ]\n",
        "            printed = False\n",
        "            for rowIndex, row in enumerate(reader):\n",
        "                text = row[textColIndex]\n",
        "                # Store all label columns independently\n",
        "                labels = [int(row[labelColIndex]) for labelColIndex in labelColIndices] # Changed line\n",
        "                # Removing aggregation logic\n",
        "                # if not printed:\n",
        "                #     print(labels)\n",
        "                #     printed = True\n",
        "\n",
        "\n",
        "                self.texts.append(text)\n",
        "                self.labels_list.append(labels)\n",
        "        self.labels = torch.tensor(self.labels_list, dtype=torch.float32) # Changed line\n",
        "        # Calculate weights for each label independently\n",
        "        pos_labels = self.labels.sum(dim=0) # Changed line\n",
        "        neg_labels = self.labels.shape[0] - pos_labels # Changed line\n",
        "        self.weights = neg_labels/pos_labels # Changed line\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "        text = self.texts[index]\n",
        "        labels = self.labels[index]\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.maxLength,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        return {\n",
        "            \"input_ids\": encoding[\"input_ids\"].flatten(),\n",
        "            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n",
        "            \"labels\": labels,\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "sEBLLDda38An"
      },
      "outputs": [],
      "source": [
        "class distilRB_base(nn.Module):\n",
        "    def __init__(self, multilabel: bool=False):\n",
        "        super(distilRB_base, self).__init__()\n",
        "        self.base_model = AutoModel.from_pretrained(\"distilbert/distilroberta-base\")\n",
        "        self.activation = nn.Linear(self.base_model.config.hidden_size, NUM_CLASSES if multilabel else 1)\n",
        "\n",
        "    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor):\n",
        "        inter = self.base_model(input_ids, attention_mask)\n",
        "        pooled = inter.pooler_output\n",
        "        y_hat = self.activation(pooled)\n",
        "        return y_hat\n",
        "\n",
        "class distilRB_hate(nn.Module):\n",
        "    def __init__(self, multilabel: bool=False):\n",
        "        super(distilRB_hate, self).__init__()\n",
        "        self.base_model = AutoModel.from_pretrained(\"distilbert/distilroberta-base\")\n",
        "        self.hate_model = AutoModelForSequenceClassification.from_pretrained(\"tomh/toxigen_roberta\")\n",
        "        for param in self.hate_model.parameters():\n",
        "            param.requires_grad = False\n",
        "        #self.hate_model output for forward is a logit of size 2\n",
        "        self.activation = nn.Linear(self.base_model.config.hidden_size + NUM_HATE, NUM_CLASSES if multilabel else 1)\n",
        "\n",
        "    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor):\n",
        "        inter = self.base_model(input_ids, attention_mask)\n",
        "        pooled = inter.pooler_output\n",
        "        hate_signal = self.hate_model(input_ids, attention_mask).logits\n",
        "        combined = torch.cat([pooled, hate_signal], dim=1)\n",
        "\n",
        "        y_hat = self.activation(combined)\n",
        "        return y_hat\n",
        "\n",
        "\n",
        "class distilRB_sem(nn.Module):\n",
        "    def __init__(self, multilabel: bool=False):\n",
        "        super(distilRB_sem, self).__init__()\n",
        "        self.base_model = AutoModel.from_pretrained(\"distilbert/distilroberta-base\")\n",
        "        self.sent_anal = AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
        "        for param in self.sent_anal.parameters():\n",
        "            param.requires_grad = False\n",
        "        #self.sentiment output for forward is a logit of size 3\n",
        "        self.activation = nn.Linear(self.base_model.config.hidden_size + NUM_SENTIMENT, NUM_CLASSES if multilabel else 1)\n",
        "\n",
        "    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor):\n",
        "        inter = self.base_model(input_ids, attention_mask)\n",
        "        pooled = inter.pooler_output\n",
        "        sentiment = self.sent_anal(input_ids, attention_mask)\n",
        "        combined = torch.cat([pooled, sentiment], dim=1)\n",
        "        y_hat = self.activation(combined)\n",
        "        return y_hat\n",
        "\n",
        "class distilRB_combine(nn.Module):\n",
        "    def __init__(self, multilabel: bool=False):\n",
        "        super(distilRB_combine, self).__init__()\n",
        "        self.base_model = AutoModel.from_pretrained(\"distilbert/distilroberta-base\")\n",
        "        self.sent_anal = AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
        "        self.hate_model = AutoModelForSequenceClassification.from_pretrained(\"tomh/toxigen_roberta\")\n",
        "        for param in self.sent_anal.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        for param in self.hate_model.parameters():\n",
        "            param.requires_grad = False\n",
        "        #self.sentiment output for forward is a logit of size 3\n",
        "        self.activation = nn.Linear(\n",
        "            self.base_model.config.hidden_size + NUM_SENTIMENT + NUM_HATE, NUM_CLASSES if multilabel else 1\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor):\n",
        "        inter = self.base_model(input_ids, attention_mask)\n",
        "        pooled = inter.pooler_output\n",
        "        sentiment = self.sent_anal(input_ids, attention_mask)\n",
        "        hate = self.hate_model(input_ids, attention_mask)\n",
        "        combined = torch.cat([pooled, sentiment, hate], dim=1)\n",
        "        y_hat = self.activation(combined)\n",
        "        return y_hat\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "R7xiIAt64FAT"
      },
      "outputs": [],
      "source": [
        "def trim(batch: dict[str, torch.tensor]):\n",
        "    masks = batch['attention_mask']\n",
        "    max_len = torch.max(torch.sum(masks, dim=1))\n",
        "\n",
        "    if batch['labels'].shape == batch['input_ids'].shape:\n",
        "        batch['labels'] = batch['labels'][:, :max_len]\n",
        "\n",
        "    batch['input_ids'] = batch['input_ids'][:, :max_len]\n",
        "    batch['attention_mask'] = batch['attention_mask'][:, :max_len]\n",
        "\n",
        "    return batch\n",
        "\n",
        "def train_model(model, dataset: Dataset, lr=1e-5, weight_decay=1e-3):\n",
        "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(lr=lr, weight_decay=weight_decay, params=model.parameters())\n",
        "\n",
        "\n",
        "    train_dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, drop_last=True, shuffle=True)\n",
        "    loss_per_epoch = []\n",
        "\n",
        "    criterion = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(EPOCHS):\n",
        "        losses = []\n",
        "        for batch in train_dataloader:\n",
        "            batch = trim(batch)\n",
        "\n",
        "            inputs = {k: v.to(device) for k, v in batch.items() if k != \"labels\"}\n",
        "            # Cast labels to float16 before loss calculation\n",
        "            labels = batch[\"labels\"].to(device) # Changed line\n",
        "            with torch.amp.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
        "                outputs = model(**inputs).squeeze()\n",
        "\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        loss_per_epoch.append(losses)\n",
        "\n",
        "    return loss_per_epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "Ecc_TeLf4RZ5"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, dataset: ToxicCommentsRobertaEvalDataset, device='cuda'):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    eval_dataloader = DataLoader(dataset, batch_size=BATCH_SIZE)  # Create DataLoader\n",
        "\n",
        "    preds = []\n",
        "    true_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in eval_dataloader:\n",
        "            inputs = {k: v.to(device) for k, v in batch.items() if k != \"labels\"}\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            with torch.amp.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
        "                outputs = model(**inputs).squeeze()\n",
        "\n",
        "            probs = torch.sigmoid(outputs)\n",
        "            pred = (probs > 0.5).int().cpu().numpy()\n",
        "\n",
        "            preds.extend(pred)  # Extend the list of predictions\n",
        "            true_labels.extend(labels.cpu().numpy())  # Extend the list of true labels\n",
        "\n",
        "    # Convert preds to a NumPy array with the correct shape\n",
        "    preds = np.array(preds)\n",
        "    true_labels = np.array(true_labels) # Ensure true_labels is an integer array\n",
        "\n",
        "    print(preds.shape)\n",
        "    print(true_labels.shape)\n",
        "\n",
        "    # Calculate overall confusion matrix\n",
        "    cm = multilabel_confusion_matrix(true_labels, preds)\n",
        "\n",
        "    f1 = sklearn.metrics.f1_score(true_labels, preds, average='macro')\n",
        "    accuracy = sklearn.metrics.accuracy_score(true_labels, preds)\n",
        "    precision = sklearn.metrics.precision_score(true_labels, preds, average='macro')\n",
        "    recall = sklearn.metrics.recall_score(true_labels, preds, average='macro')\n",
        "\n",
        "    metrics = {\n",
        "        'accuracy': float(accuracy),\n",
        "        'precision': float(precision),\n",
        "        'recall': float(recall),\n",
        "        'f1': float(f1)\n",
        "    }\n",
        "\n",
        "    return metrics, preds, true_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U9Ins6Y-4zdU",
        "outputId": "4795253b-2d86-4c3e-d6f6-6aa25359c3b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2572.77587890625\n",
            "2790.0\n",
            "2572.77587890625\n",
            "2624.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-53-0eb77bb2d325>:3: FutureWarning: `torch.cuda.memory_cached` has been renamed to `torch.cuda.memory_reserved`\n",
            "  print(torch.cuda.memory_cached()/1024**2)\n"
          ]
        }
      ],
      "source": [
        "def memory_stats():\n",
        "    print(torch.cuda.memory_allocated()/1024**2)\n",
        "    print(torch.cuda.memory_cached()/1024**2)\n",
        "\n",
        "\n",
        "if __name__ == '__main__' :\n",
        "    memory_stats()\n",
        "    torch.cuda.empty_cache()\n",
        "    memory_stats()\n",
        "    tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')\n",
        "    toxicDataset = ToxicCommentsRobertaDataset(trainDatasetPath, 'comment_text', tokenizer=tokenizer)\n",
        "\n",
        "    # if len(toxicDataset) > 0:\n",
        "    #     print(f\"\\nLength of dataset: {len(toxicDataset)}\")\n",
        "    #     print(toxicDataset[1])\n",
        "    # Instantiate the model you want to train (e.g., distilRB_base)\n",
        "    model = distilRB_hate(multilabel=True)\n",
        "\n",
        "\n",
        "    # Train the model\n",
        "    loss_per_epoch = train_model(model, toxicDataset)\n",
        "\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ToxicCommentsRobertaEvalDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer, maxLength=512):\n",
        "        self.df = df\n",
        "        self.tokenizer = tokenizer\n",
        "        self.maxLength = maxLength\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        text = self.df.iloc[index]['comment_text']\n",
        "        labels = self.df.iloc[index][['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].values.astype(int)\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.maxLength,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        return {\n",
        "            \"input_ids\": encoding[\"input_ids\"].flatten(),\n",
        "            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n",
        "            \"labels\": torch.tensor(labels, dtype=torch.float32),\n",
        "        }"
      ],
      "metadata": {
        "id": "U2azpzM0fz9w"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test = pd.read_csv('test.csv', quotechar='\"')\n",
        "df_labels = pd.read_csv('test_labels.csv')\n",
        "\n",
        "df_merged = pd.merge(df_test, df_labels, on='id', how='left')\n",
        "df_filtered = df_merged[df_merged['toxic'] != -1]  # Filter out rows with -1 labels\n",
        "\n",
        "# Create the evaluation dataset\n",
        "eval_dataset = ToxicCommentsRobertaEvalDataset(df_filtered, tokenizer)\n",
        "\n",
        "# Evaluate the model using the DataLoader\n",
        "metrics, preds, true_labels, cm  = evaluate_model(model, eval_dataset)\n",
        "print(metrics)"
      ],
      "metadata": {
        "id": "XQWxlXQWpc1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W81sz5ITfjqx"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def visualize_metrics(df_filtered, metrics, loss_per_epoch, true_labels, preds, cm):    '''\n",
        "        Visualizes all metrics after training\n",
        "    '''\n",
        "    # 1. Distribution of Labels (Updated)\n",
        "    # --- Melt the DataFrame to create a 'labels' column ---\n",
        "    df_melted = df_filtered.melt(\n",
        "        id_vars=['id', 'comment_text'], # Keep these columns as identifiers\n",
        "        value_vars=['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'],\n",
        "        var_name='labels', # Name of the new column for label types\n",
        "        value_name='label_value' # Name of the new column for label values (0 or 1)\n",
        "    )\n",
        "\n",
        "    # --- Filter out rows where label_value is -1 (these were the ones we filtered before) ---\n",
        "    df_melted = df_melted[df_melted['label_value'] != -1]\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    # --- Count only positive samples (label_value == 1) ---\n",
        "    sns.countplot(x='labels', data=df_melted[df_melted['label_value'] == 1])\n",
        "    plt.title('Distribution of Labels in Test Data')\n",
        "    plt.xlabel('Label (0: Non-Toxic, 1: Toxic)')\n",
        "    plt.ylabel('Count')\n",
        "    plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability\n",
        "    plt.show()\n",
        "\n",
        "    # 2. Metrics Visualization (Updated)\n",
        "    # Calculate F1 scores for each label\n",
        "    label_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']  # Label names\n",
        "    f1_scores = sklearn.metrics.f1_score(true_labels, preds, average=None) # Calculate F1 for each label\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.barplot(x=label_names, y=f1_scores)\n",
        "    plt.title('F1 Score per Label')\n",
        "    plt.ylim(0, 1)\n",
        "    plt.ylabel('F1 Score')\n",
        "    plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    # 3. Confusion Matrix Visualization (Updated for Focus on Positive Cases with Correct Labels)\n",
        "    label_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
        "\n",
        "    # --- Plot Confusion Matrix for Positive Cases ---\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "    axes = axes.ravel()\n",
        "\n",
        "    for i, label in enumerate(label_names):\n",
        "        # Extract TP and FP from the confusion matrix\n",
        "        TP = cm[i, 1, 1]  # True Positives\n",
        "        FP = cm[i, 0, 1]  # False Positives\n",
        "\n",
        "        # Create a custom confusion matrix with only TP and FP\n",
        "        custom_cm = np.array([[FP, TP]]) # Correct custom_cm shape\n",
        "\n",
        "        sns.heatmap(custom_cm, annot=True, fmt='d', cmap='Blues',\n",
        "                    xticklabels=['Predicted Negative', 'Predicted Positive'], # Axis labels remain same\n",
        "                    yticklabels=['Actual Positive'], # update y axis label\n",
        "                    ax=axes[i])\n",
        "        axes[i].set_title(f'Confusion Matrix for Label: {label} (Positive Cases)')\n",
        "        axes[i].set_xlabel('Predicted Label')\n",
        "        axes[i].set_ylabel('True Label')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    # 4.  Loss Curve (if you saved the loss_per_epoch during training)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    for epoch_losses in loss_per_epoch:\n",
        "        plt.plot(range(len(epoch_losses)), epoch_losses)\n",
        "    plt.title('Training Loss per Epoch')\n",
        "    plt.xlabel('Batch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_metrics(df_filtered, metrics, loss_per_epoch, true_labels, preds, cm)  # Pass the overall confusion matrix"
      ],
      "metadata": {
        "id": "GpLS5yAUzsDv"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}